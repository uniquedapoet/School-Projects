{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    "1. Obtain data from Kaggle and lyrics from Genius\n",
    "2. Create Database and Class for songs\n",
    "    - Database will store data which should be retrieved with Class when called.\n",
    "3. Train Model on genre using audio features and lyrics\n",
    "4. Load songs and genre into database\n",
    "5. Create user interactions through discord \n",
    "    - Allow songs within database to get genre from there\n",
    "    - Allow new songs to be run through model\n",
    "    - Save new songs to databse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import used libraries\n",
    "\n",
    "import pandas as pd\n",
    "from project_functions import *\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from collections import Counter\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in datasets from csv and combine them and drop duplicates\n",
    "\n",
    "spotify_2000 = pd.read_csv('data/Spotify-2000.csv', index_col=0)\n",
    "spotify_tracks = pd.read_csv('data/Spotify-Tracks.csv', index_col=0)\n",
    "spotify_2000 = spotify_2000[~spotify_2000['Title'].isin(spotify_tracks['Title'])]\n",
    "spotify_2000 = spotify_2000.drop(columns=['Year','Beats Per Minute (BPM)'])\n",
    "spotify_tracks = spotify_tracks[spotify_2000.columns]\n",
    "data = pd.concat([spotify_tracks, spotify_2000], ignore_index=True)\n",
    "data = data.drop_duplicates(subset=['Title','Artist'], keep='first')\n",
    "# data.to_csv('data/All-Songs.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter niche genres into more general ones for training\n",
    "data = filter_genres(data)\n",
    "data.to_csv('data/All-Songs.csv')\n",
    "data['Genre'].value_counts().to_csv('data/Genre-Counts.csv')\n",
    "data['Top Genre'].value_counts().to_csv('data/Original-Genres.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See file `project_functions.py` for this step!!\n",
    "    - Songs filtered to only english songs\n",
    "    - Lyrics retrieved for each song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to database and store data\n",
    "data = pd.read_csv('data/English-Songs.csv', index_col=0)\n",
    "con = sqlite3.connect('data/spotify.db')\n",
    "cursor = con.cursor()\n",
    "cursor.execute('DROP TABLE IF EXISTS songs')\n",
    "data.to_sql('songs', con, if_exists='append', index=False)\n",
    "con.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Song object for each row in the dataset\n",
    "songs = []  \n",
    "for i, row in data.iterrows():\n",
    "    song = Song(\n",
    "        name=row['Title'],\n",
    "        artist=row['Artist'],\n",
    "        energy=row['Energy'],\n",
    "        danceability=row['Danceability'],\n",
    "        loudness=row['Loudness (dB)'],\n",
    "        liveness=row['Liveness'],\n",
    "        valence=row['Valence'],\n",
    "        acousticness=row['Acousticness'],\n",
    "        speechiness=row['Speechiness'],\n",
    "        popularity=row['Popularity']\n",
    "    )\n",
    "    songs.append(song)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution: Counter({3: 3549, 9: 2261, 8: 1943, 2: 1645, 7: 1589, 0: 1511, 1: 1129, 5: 554, 4: 209, 6: 176})\n",
      "Training distribution: Counter({3: 2839, 9: 1809, 8: 1554, 2: 1316, 7: 1271, 0: 1209, 1: 903, 5: 443, 4: 167, 6: 141})\n",
      "Testing distribution: Counter({3: 710, 9: 452, 8: 389, 2: 329, 7: 318, 0: 302, 1: 226, 5: 111, 4: 42, 6: 35})\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns=['Artist','Top Genre','Genre', 'Title', 'Length (Duration)'])  \n",
    "y = data['Genre']  \n",
    "genre_counts = y.value_counts()\n",
    "rare_genres = genre_counts[genre_counts <= 1].index\n",
    "y = y.apply(lambda genre: 'Other' if genre in rare_genres else genre)\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == 'object':  # If the column contains strings\n",
    "        X[col] = X[col].str.replace(',', '').astype(float)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\\\n",
    "\n",
    "\n",
    "print(\"Original distribution:\", Counter(y))\n",
    "print(\"Training distribution:\", Counter(y_train))\n",
    "print(\"Testing distribution:\", Counter(y_test))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.2, 'max_depth': 5, 'n_estimators': 150, 'subsample': 0.8}\n",
      "Best Accuracy: 0.624785444558874\n",
      "Test Accuracy: 0.6108442004118051\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(random_state=42)\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'learning_rate': [0.01, 0.2,.3,],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.8,]\n",
    "}\n",
    "\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Perform the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n",
    "# Evaluate on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "test_accuracy = best_model.score(X_test, y_test)\n",
    "print(\"Test Accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n",
      "WARNING:absl:There is a known slowdown when using v2.11+ Keras optimizers on M1/M2 Macs. Falling back to the legacy Keras optimizer, i.e., `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_41 (Dense)            (None, 256)               2304      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 256)               0         \n",
      "                                                                 \n",
      " dense_42 (Dense)            (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_44 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_45 (Dense)            (None, 13)                429       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 45,965\n",
      "Trainable params: 45,965\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(256, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dropout(0.3),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(len(np.unique(y)), activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1648/1648 [==============================] - 7s 4ms/step - loss: 2.0608 - accuracy: 0.3120 - val_loss: 1.8875 - val_accuracy: 0.3463\n",
      "Epoch 2/50\n",
      "1648/1648 [==============================] - 4s 3ms/step - loss: 1.9008 - accuracy: 0.3397 - val_loss: 1.8116 - val_accuracy: 0.3660\n",
      "Epoch 3/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.8315 - accuracy: 0.3538 - val_loss: 1.7479 - val_accuracy: 0.3777\n",
      "Epoch 4/50\n",
      "1648/1648 [==============================] - 5s 3ms/step - loss: 1.7914 - accuracy: 0.3624 - val_loss: 1.7080 - val_accuracy: 0.3878\n",
      "Epoch 5/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.7564 - accuracy: 0.3723 - val_loss: 1.6756 - val_accuracy: 0.3983\n",
      "Epoch 6/50\n",
      "1648/1648 [==============================] - 5s 3ms/step - loss: 1.7376 - accuracy: 0.3763 - val_loss: 1.6659 - val_accuracy: 0.4022\n",
      "Epoch 7/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.7193 - accuracy: 0.3844 - val_loss: 1.6429 - val_accuracy: 0.4045\n",
      "Epoch 8/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.7114 - accuracy: 0.3876 - val_loss: 1.6479 - val_accuracy: 0.4080\n",
      "Epoch 9/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6994 - accuracy: 0.3908 - val_loss: 1.6278 - val_accuracy: 0.4153\n",
      "Epoch 10/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6910 - accuracy: 0.3943 - val_loss: 1.6083 - val_accuracy: 0.4234\n",
      "Epoch 11/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6867 - accuracy: 0.3959 - val_loss: 1.6073 - val_accuracy: 0.4223\n",
      "Epoch 12/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6835 - accuracy: 0.3964 - val_loss: 1.6139 - val_accuracy: 0.4218\n",
      "Epoch 13/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6774 - accuracy: 0.4004 - val_loss: 1.5976 - val_accuracy: 0.4309\n",
      "Epoch 14/50\n",
      "1648/1648 [==============================] - 4s 3ms/step - loss: 1.6753 - accuracy: 0.3985 - val_loss: 1.6013 - val_accuracy: 0.4253\n",
      "Epoch 15/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6672 - accuracy: 0.4008 - val_loss: 1.5932 - val_accuracy: 0.4233\n",
      "Epoch 16/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6653 - accuracy: 0.4021 - val_loss: 1.5893 - val_accuracy: 0.4282\n",
      "Epoch 17/50\n",
      "1648/1648 [==============================] - 4s 3ms/step - loss: 1.6613 - accuracy: 0.4050 - val_loss: 1.5966 - val_accuracy: 0.4246\n",
      "Epoch 18/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6597 - accuracy: 0.4043 - val_loss: 1.6097 - val_accuracy: 0.4218\n",
      "Epoch 19/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6516 - accuracy: 0.4060 - val_loss: 1.5775 - val_accuracy: 0.4357\n",
      "Epoch 20/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6551 - accuracy: 0.4045 - val_loss: 1.5838 - val_accuracy: 0.4351\n",
      "Epoch 21/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6525 - accuracy: 0.4062 - val_loss: 1.5829 - val_accuracy: 0.4302\n",
      "Epoch 22/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6515 - accuracy: 0.4065 - val_loss: 1.5828 - val_accuracy: 0.4328\n",
      "Epoch 23/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6500 - accuracy: 0.4091 - val_loss: 1.5814 - val_accuracy: 0.4333\n",
      "Epoch 24/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6440 - accuracy: 0.4077 - val_loss: 1.5756 - val_accuracy: 0.4382\n",
      "Epoch 25/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6453 - accuracy: 0.4094 - val_loss: 1.5767 - val_accuracy: 0.4341\n",
      "Epoch 26/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6437 - accuracy: 0.4087 - val_loss: 1.5750 - val_accuracy: 0.4317\n",
      "Epoch 27/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6424 - accuracy: 0.4107 - val_loss: 1.5749 - val_accuracy: 0.4332\n",
      "Epoch 28/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6387 - accuracy: 0.4139 - val_loss: 1.5712 - val_accuracy: 0.4388\n",
      "Epoch 29/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6426 - accuracy: 0.4092 - val_loss: 1.5841 - val_accuracy: 0.4313\n",
      "Epoch 30/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6387 - accuracy: 0.4131 - val_loss: 1.5717 - val_accuracy: 0.4376\n",
      "Epoch 31/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6337 - accuracy: 0.4124 - val_loss: 1.5698 - val_accuracy: 0.4410\n",
      "Epoch 32/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6371 - accuracy: 0.4141 - val_loss: 1.5595 - val_accuracy: 0.4371\n",
      "Epoch 33/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6336 - accuracy: 0.4142 - val_loss: 1.5734 - val_accuracy: 0.4372\n",
      "Epoch 34/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6329 - accuracy: 0.4128 - val_loss: 1.5728 - val_accuracy: 0.4371\n",
      "Epoch 35/50\n",
      "1648/1648 [==============================] - 3s 2ms/step - loss: 1.6310 - accuracy: 0.4151 - val_loss: 1.5656 - val_accuracy: 0.4434\n",
      "Epoch 36/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6352 - accuracy: 0.4128 - val_loss: 1.5714 - val_accuracy: 0.4389\n",
      "Epoch 37/50\n",
      "1648/1648 [==============================] - 4s 2ms/step - loss: 1.6312 - accuracy: 0.4153 - val_loss: 1.5691 - val_accuracy: 0.4375\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x36980a950>"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "515/515 [==============================] - 0s 663us/step - loss: 1.5668 - accuracy: 0.4395\n",
      "Test Accuracy: 0.44\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
