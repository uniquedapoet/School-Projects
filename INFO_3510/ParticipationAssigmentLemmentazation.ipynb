{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Access Token: ISsvY5L6Ev-qmCX5u2jgWC4BpJbn0-cDqbaEgf-SgukJWIvKwdZh4yS1g1kEff__\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "def get_access_token(url, client_id, client_secret):\n",
    "    url = url\n",
    "    data = {\n",
    "        'client_id': client_id,\n",
    "        'client_secret': client_secret,\n",
    "        'grant_type': 'client_credentials'\n",
    "    }\n",
    "    response = requests.post(url, data=data)\n",
    "    response_data = response.json()\n",
    "    if response.status_code != 200:\n",
    "        print('Error:', response_data['error'])\n",
    "        return None\n",
    "    else:\n",
    "        print('Access Token:', response_data['access_token'])\n",
    "        return response_data['access_token']\n",
    "load_dotenv('3510.env')\n",
    "client_id = os.getenv('GENIUS_CLIENT_ID')\n",
    "client_secret = os.getenv('GENIUS_SECRET')\n",
    "access_token = get_access_token(url='https://api.genius.com/oauth/token', client_id=client_id, client_secret=client_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://api.genius.com/search'\n",
    "headers = {\n",
    "    'Authorization': f'Bearer {access_token}'\n",
    "}\n",
    "params = {\n",
    "    'q': 'Heaven Knows Iâ€™m Miserable Now',\n",
    "}\n",
    "response = requests.get(url, headers=headers, params=params).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://genius.com/The-smiths-heaven-knows-im-miserable-now-lyrics\n"
     ]
    }
   ],
   "source": [
    "id = response['response']['hits'][0]['result']['id']\n",
    "url = f'https://api.genius.com/songs/{id}'\n",
    "songRequest = requests.get(url, headers=headers).json()\n",
    "lyricsurl = songRequest['response']['song']['url']\n",
    "print(lyricsurl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Verse 1]I was happy in the haze of a drunken hourBut heaven knows I'm miserable nowI was looking for a job and then I found a jobAnd heaven knows I'm miserable now[Chorus]In my life, why do I give valuable timeTo people who don't careIf I live or die?[Verse 2]Two lovers entwined pass me byAnd heaven knows I'm miserable nowI was looking for a job and then I found a jobAnd heaven knows I'm miserable now[Chorus]In my life, oh, why do I give valuable timeTo people who don't careIf I live or I die?[Verse 3]What she asked of me at the end of the dayCaligula would have blushed\"Oh, you've been in the house too long,\" she saidAnd I naturally fled\n",
      "[Chorus]In my life, why do I smileAt people who I'd much ratherKick in the eye?[Verse 4]I was happy in the haze of a drunken hourBut heaven knows I'm miserable now\"Oh, you've been in the house too long,\" she saidAnd I naturally fled[Chorus]In my life, oh, why do I give valuable timeTo people who don't careIf I live or I die?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_lyrics_from_url(url):\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        lyrics_div = soup.find('div', class_='lyrics') or soup.find_all('div', {'data-lyrics-container': 'true'})\n",
    "        \n",
    "        if lyrics_div:\n",
    "            lyrics = \"\\n\".join([div.get_text() for div in lyrics_div])\n",
    "            return lyrics\n",
    "        else:\n",
    "            return \"Lyrics not found on the page.\"\n",
    "    else:\n",
    "        print(f\"Error fetching the song page: {response.status_code}\")\n",
    "        return None\n",
    "\n",
    "lyrics = scrape_lyrics_from_url(lyricsurl)\n",
    "print(lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization and POS tagging sentence by sentence:\n",
      "[('I', 'PRP'), ('was', 'VBD'), ('happy', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('haze', 'NN'), ('of', 'IN'), ('a', 'DT'), ('drunken', 'JJ'), ('hourBut', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowI', 'NN'), ('was', 'VBD'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('job', 'NN'), ('and', 'CC'), ('then', 'RB'), ('I', 'PRP'), ('found', 'VBD'), ('a', 'DT'), ('jobAnd', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowIn', 'NN'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('give', 'VB'), ('valuable', 'JJ'), ('timeTo', 'JJ'), ('people', 'NNS'), ('who', 'WP'), ('do', 'VBP'), (\"n't\", 'RB'), ('careIf', 'VB'), ('I', 'PRP'), ('live', 'VBP'), ('or', 'CC'), ('die', 'VB'), ('?', '.'), ('Two', 'CD'), ('lovers', 'NNS'), ('entwined', 'VBD'), ('pass', 'IN'), ('me', 'PRP'), ('byAnd', 'VB'), ('heaven', 'JJ'), ('knows', 'NNS'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowI', 'NN'), ('was', 'VBD'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('job', 'NN'), ('and', 'CC'), ('then', 'RB'), ('I', 'PRP'), ('found', 'VBD'), ('a', 'DT'), ('jobAnd', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowIn', 'NN'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('oh', 'UH'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('give', 'VB'), ('valuable', 'JJ'), ('timeTo', 'JJ'), ('people', 'NNS'), ('who', 'WP'), ('do', 'VBP'), (\"n't\", 'RB'), ('careIf', 'VB'), ('I', 'PRP'), ('live', 'VBP'), ('or', 'CC'), ('I', 'PRP'), ('die', 'VBP'), ('?', '.'), ('What', 'WP'), ('she', 'PRP'), ('asked', 'VBD'), ('of', 'IN'), ('me', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('the', 'DT'), ('dayCaligula', 'NN'), ('would', 'MD'), ('have', 'VB'), ('blushed', 'VBN'), (\"''\", \"''\"), ('Oh', 'UH'), (',', ','), ('you', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('house', 'NN'), ('too', 'RB'), ('long', 'RB'), (',', ','), (\"''\", \"''\"), ('she', 'PRP'), ('saidAnd', 'VBD'), ('I', 'PRP'), ('naturally', 'RB'), ('fled', 'VBD'), ('In', 'IN'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('smileAt', 'VBP'), ('people', 'NNS'), ('who', 'WP'), ('I', 'PRP'), (\"'d\", 'MD'), ('much', 'RB'), ('ratherKick', 'VB'), ('in', 'IN'), ('the', 'DT'), ('eye', 'NN'), ('?', '.'), ('I', 'PRP'), ('was', 'VBD'), ('happy', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('haze', 'NN'), ('of', 'IN'), ('a', 'DT'), ('drunken', 'JJ'), ('hourBut', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('now', 'RB'), (\"''\", \"''\"), ('Oh', 'UH'), (',', ','), ('you', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('house', 'NN'), ('too', 'RB'), ('long', 'RB'), (',', ','), (\"''\", \"''\"), ('she', 'PRP'), ('saidAnd', 'VBD'), ('I', 'PRP'), ('naturally', 'RB'), ('fledIn', 'VB'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('oh', 'UH'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('give', 'VB'), ('valuable', 'JJ'), ('timeTo', 'JJ'), ('people', 'NNS'), ('who', 'WP'), ('do', 'VBP'), (\"n't\", 'RB'), ('careIf', 'VB'), ('I', 'PRP'), ('live', 'VBP'), ('or', 'CC'), ('I', 'PRP'), ('die', 'VBP'), ('?', '.')]\n",
      "\n",
      "Tokenization and POS tagging all at once:\n",
      "[('I', 'PRP'), ('was', 'VBD'), ('happy', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('haze', 'NN'), ('of', 'IN'), ('a', 'DT'), ('drunken', 'JJ'), ('hourBut', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowI', 'NN'), ('was', 'VBD'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('job', 'NN'), ('and', 'CC'), ('then', 'RB'), ('I', 'PRP'), ('found', 'VBD'), ('a', 'DT'), ('jobAnd', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowIn', 'NN'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('give', 'VB'), ('valuable', 'JJ'), ('timeTo', 'JJ'), ('people', 'NNS'), ('who', 'WP'), ('do', 'VBP'), (\"n't\", 'RB'), ('careIf', 'VB'), ('I', 'PRP'), ('live', 'VBP'), ('or', 'CC'), ('die', 'VB'), ('?', '.'), ('Two', 'CD'), ('lovers', 'NNS'), ('entwined', 'VBD'), ('pass', 'IN'), ('me', 'PRP'), ('byAnd', 'VB'), ('heaven', 'JJ'), ('knows', 'NNS'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowI', 'NN'), ('was', 'VBD'), ('looking', 'VBG'), ('for', 'IN'), ('a', 'DT'), ('job', 'NN'), ('and', 'CC'), ('then', 'RB'), ('I', 'PRP'), ('found', 'VBD'), ('a', 'DT'), ('jobAnd', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('nowIn', 'NN'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('oh', 'UH'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('give', 'VB'), ('valuable', 'JJ'), ('timeTo', 'JJ'), ('people', 'NNS'), ('who', 'WP'), ('do', 'VBP'), (\"n't\", 'RB'), ('careIf', 'VB'), ('I', 'PRP'), ('live', 'VBP'), ('or', 'CC'), ('I', 'PRP'), ('die', 'VBP'), ('?', '.'), ('What', 'WP'), ('she', 'PRP'), ('asked', 'VBD'), ('of', 'IN'), ('me', 'PRP'), ('at', 'IN'), ('the', 'DT'), ('end', 'NN'), ('of', 'IN'), ('the', 'DT'), ('dayCaligula', 'NN'), ('would', 'MD'), ('have', 'VB'), ('blushed', 'VBN'), (\"''\", \"''\"), ('Oh', 'UH'), (',', ','), ('you', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('house', 'NN'), ('too', 'RB'), ('long', 'RB'), (',', ','), (\"''\", \"''\"), ('she', 'PRP'), ('saidAnd', 'VBD'), ('I', 'PRP'), ('naturally', 'RB'), ('fled', 'VBD'), ('In', 'IN'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('smileAt', 'VBP'), ('people', 'NNS'), ('who', 'WP'), ('I', 'PRP'), (\"'d\", 'MD'), ('much', 'RB'), ('ratherKick', 'VB'), ('in', 'IN'), ('the', 'DT'), ('eye', 'NN'), ('?', '.'), ('I', 'PRP'), ('was', 'VBD'), ('happy', 'JJ'), ('in', 'IN'), ('the', 'DT'), ('haze', 'NN'), ('of', 'IN'), ('a', 'DT'), ('drunken', 'JJ'), ('hourBut', 'NN'), ('heaven', 'NN'), ('knows', 'VBZ'), ('I', 'PRP'), (\"'m\", 'VBP'), ('miserable', 'JJ'), ('now', 'RB'), (\"''\", \"''\"), ('Oh', 'UH'), (',', ','), ('you', 'PRP'), (\"'ve\", 'VBP'), ('been', 'VBN'), ('in', 'IN'), ('the', 'DT'), ('house', 'NN'), ('too', 'RB'), ('long', 'RB'), (',', ','), (\"''\", \"''\"), ('she', 'PRP'), ('saidAnd', 'VBD'), ('I', 'PRP'), ('naturally', 'RB'), ('fledIn', 'VB'), ('my', 'PRP$'), ('life', 'NN'), (',', ','), ('oh', 'UH'), (',', ','), ('why', 'WRB'), ('do', 'VBP'), ('I', 'PRP'), ('give', 'VB'), ('valuable', 'JJ'), ('timeTo', 'JJ'), ('people', 'NNS'), ('who', 'WP'), ('do', 'VBP'), (\"n't\", 'RB'), ('careIf', 'VB'), ('I', 'PRP'), ('live', 'VBP'), ('or', 'CC'), ('I', 'PRP'), ('die', 'VBP'), ('?', '.')]\n",
      "\n",
      "Lemmatized sentence by sentence:\n",
      "['I', 'be', 'happy', 'in', 'the', 'haze', 'of', 'a', 'drunken', 'hourBut', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowI', 'be', 'look', 'for', 'a', 'job', 'and', 'then', 'I', 'find', 'a', 'jobAnd', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowIn', 'my', 'life', ',', 'why', 'do', 'I', 'give', 'valuable', 'timeTo', 'people', 'who', 'do', \"n't\", 'careIf', 'I', 'live', 'or', 'die', '?', 'Two', 'lover', 'entwine', 'pas', 'me', 'byAnd', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowI', 'be', 'look', 'for', 'a', 'job', 'and', 'then', 'I', 'find', 'a', 'jobAnd', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowIn', 'my', 'life', ',', 'oh', ',', 'why', 'do', 'I', 'give', 'valuable', 'timeTo', 'people', 'who', 'do', \"n't\", 'careIf', 'I', 'live', 'or', 'I', 'die', '?', 'What', 'she', 'ask', 'of', 'me', 'at', 'the', 'end', 'of', 'the', 'dayCaligula', 'would', 'have', 'blush', \"''\", 'Oh', ',', 'you', \"'ve\", 'be', 'in', 'the', 'house', 'too', 'long', ',', \"''\", 'she', 'saidAnd', 'I', 'naturally', 'flee', 'In', 'my', 'life', ',', 'why', 'do', 'I', 'smileAt', 'people', 'who', 'I', \"'d\", 'much', 'ratherKick', 'in', 'the', 'eye', '?', 'I', 'be', 'happy', 'in', 'the', 'haze', 'of', 'a', 'drunken', 'hourBut', 'heaven', 'know', 'I', \"'m\", 'miserable', 'now', \"''\", 'Oh', ',', 'you', \"'ve\", 'be', 'in', 'the', 'house', 'too', 'long', ',', \"''\", 'she', 'saidAnd', 'I', 'naturally', 'fledIn', 'my', 'life', ',', 'oh', ',', 'why', 'do', 'I', 'give', 'valuable', 'timeTo', 'people', 'who', 'do', \"n't\", 'careIf', 'I', 'live', 'or', 'I', 'die', '?']\n",
      "\n",
      "Lemmatized all at once:\n",
      "['I', 'be', 'happy', 'in', 'the', 'haze', 'of', 'a', 'drunken', 'hourBut', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowI', 'be', 'look', 'for', 'a', 'job', 'and', 'then', 'I', 'find', 'a', 'jobAnd', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowIn', 'my', 'life', ',', 'why', 'do', 'I', 'give', 'valuable', 'timeTo', 'people', 'who', 'do', \"n't\", 'careIf', 'I', 'live', 'or', 'die', '?', 'Two', 'lover', 'entwine', 'pas', 'me', 'byAnd', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowI', 'be', 'look', 'for', 'a', 'job', 'and', 'then', 'I', 'find', 'a', 'jobAnd', 'heaven', 'know', 'I', \"'m\", 'miserable', 'nowIn', 'my', 'life', ',', 'oh', ',', 'why', 'do', 'I', 'give', 'valuable', 'timeTo', 'people', 'who', 'do', \"n't\", 'careIf', 'I', 'live', 'or', 'I', 'die', '?', 'What', 'she', 'ask', 'of', 'me', 'at', 'the', 'end', 'of', 'the', 'dayCaligula', 'would', 'have', 'blush', \"''\", 'Oh', ',', 'you', \"'ve\", 'be', 'in', 'the', 'house', 'too', 'long', ',', \"''\", 'she', 'saidAnd', 'I', 'naturally', 'flee', 'In', 'my', 'life', ',', 'why', 'do', 'I', 'smileAt', 'people', 'who', 'I', \"'d\", 'much', 'ratherKick', 'in', 'the', 'eye', '?', 'I', 'be', 'happy', 'in', 'the', 'haze', 'of', 'a', 'drunken', 'hourBut', 'heaven', 'know', 'I', \"'m\", 'miserable', 'now', \"''\", 'Oh', ',', 'you', \"'ve\", 'be', 'in', 'the', 'house', 'too', 'long', ',', \"''\", 'she', 'saidAnd', 'I', 'naturally', 'fledIn', 'my', 'life', ',', 'oh', ',', 'why', 'do', 'I', 'give', 'valuable', 'timeTo', 'people', 'who', 'do', \"n't\", 'careIf', 'I', 'live', 'or', 'I', 'die', '?']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "pattern = r'\\[(.*?)\\]'\n",
    "lyrics = re.sub(pattern, '. ', lyrics)\n",
    "\n",
    "# Tokenize the lyrics sentence-by-sentence\n",
    "sentences = sent_tokenize(lyrics)\n",
    "sentence_by_sentence_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "sentence_by_sentence_pos = [pos_tag(sentence) for sentence in sentence_by_sentence_tokens]\n",
    "\n",
    "# Tokenize all at once\n",
    "all_at_once_tokens = word_tokenize(lyrics)\n",
    "all_at_once_pos = pos_tag(all_at_once_tokens)\n",
    "\n",
    "# Function to map NLTK POS tag to WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Lemmatize sentence-by-sentence\n",
    "sentence_by_sentence_lemmatized = []\n",
    "for sentence in sentence_by_sentence_pos:\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in sentence:\n",
    "        wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "        lemmatized_word = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "        lemmatized_sentence.append(lemmatized_word)\n",
    "    sentence_by_sentence_lemmatized.append(lemmatized_sentence)\n",
    "\n",
    "# Lemmatize all at once\n",
    "all_at_once_lemmatized = []\n",
    "for word, tag in all_at_once_pos:\n",
    "    wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN\n",
    "    lemmatized_word = lemmatizer.lemmatize(word, wordnet_pos)\n",
    "    all_at_once_lemmatized.append(lemmatized_word)\n",
    "\n",
    "print(\"Tokenization and POS tagging sentence by sentence:\")\n",
    "for sentence in sentence_by_sentence_pos:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nTokenization and POS tagging all at once:\")\n",
    "print(all_at_once_pos)\n",
    "\n",
    "print(\"\\nLemmatized sentence by sentence:\")\n",
    "for sentence in sentence_by_sentence_lemmatized:\n",
    "    print(sentence)\n",
    "\n",
    "print(\"\\nLemmatized all at once:\")\n",
    "print(all_at_once_lemmatized)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I -> I\n",
      "was -> be\n",
      "happy -> happy\n",
      "in -> in\n",
      "the -> the\n",
      "haze -> haze\n",
      "of -> of\n",
      "a -> a\n",
      "drunken -> drunken\n",
      "hourBut -> hourBut\n",
      "heaven -> heaven\n",
      "knows -> know\n",
      "I -> I\n",
      "'m -> be\n",
      "miserable -> miserable\n",
      "nowI -> nowI\n",
      "was -> be\n",
      "looking -> look\n",
      "for -> for\n",
      "a -> a\n",
      "job -> job\n",
      "and -> and\n",
      "then -> then\n",
      "I -> I\n",
      "found -> find\n",
      "a -> a\n",
      "jobAnd -> jobAnd\n",
      "heaven -> heaven\n",
      "knows -> know\n",
      "I -> I\n",
      "'m -> be\n",
      "miserable -> miserable\n",
      "nowIn -> nowin\n",
      "my -> my\n",
      "life -> life\n",
      ", -> ,\n",
      "why -> why\n",
      "do -> do\n",
      "I -> I\n",
      "give -> give\n",
      "valuable -> valuable\n",
      "timeTo -> timeto\n",
      "people -> people\n",
      "who -> who\n",
      "do -> do\n",
      "n't -> not\n",
      "careIf -> careif\n",
      "I -> I\n",
      "live -> live\n",
      "or -> or\n",
      "die?Two -> die?two\n",
      "lovers -> lover\n",
      "entwined -> entwine\n",
      "pass -> pass\n",
      "me -> I\n",
      "byAnd -> byAnd\n",
      "heaven -> heaven\n",
      "knows -> know\n",
      "I -> I\n",
      "'m -> be\n",
      "miserable -> miserable\n",
      "nowI -> nowI\n",
      "was -> be\n",
      "looking -> look\n",
      "for -> for\n",
      "a -> a\n",
      "job -> job\n",
      "and -> and\n",
      "then -> then\n",
      "I -> I\n",
      "found -> find\n",
      "a -> a\n",
      "jobAnd -> jobAnd\n",
      "heaven -> heaven\n",
      "knows -> know\n",
      "I -> I\n",
      "'m -> be\n",
      "miserable -> miserable\n",
      "nowIn -> nowin\n",
      "my -> my\n",
      "life -> life\n",
      ", -> ,\n",
      "oh -> oh\n",
      ", -> ,\n",
      "why -> why\n",
      "do -> do\n",
      "I -> I\n",
      "give -> give\n",
      "valuable -> valuable\n",
      "timeTo -> timeto\n",
      "people -> people\n",
      "who -> who\n",
      "do -> do\n",
      "n't -> not\n",
      "careIf -> careif\n",
      "I -> I\n",
      "live -> live\n",
      "or -> or\n",
      "I -> I\n",
      "die?What -> die?what\n",
      "she -> she\n",
      "asked -> ask\n",
      "of -> of\n",
      "me -> I\n",
      "at -> at\n",
      "the -> the\n",
      "end -> end\n",
      "of -> of\n",
      "the -> the\n",
      "dayCaligula -> daycaligula\n",
      "would -> would\n",
      "have -> have\n",
      "blushed\"Oh -> blushed\"oh\n",
      ", -> ,\n",
      "you -> you\n",
      "'ve -> have\n",
      "been -> be\n",
      "in -> in\n",
      "the -> the\n",
      "house -> house\n",
      "too -> too\n",
      "long -> long\n",
      ", -> ,\n",
      "\" -> \"\n",
      "she -> she\n",
      "saidAnd -> saidand\n",
      "I -> I\n",
      "naturally -> naturally\n",
      "fled -> flee\n",
      "\n",
      " -> \n",
      "\n",
      "In -> in\n",
      "my -> my\n",
      "life -> life\n",
      ", -> ,\n",
      "why -> why\n",
      "do -> do\n",
      "I -> I\n",
      "smileAt -> smileat\n",
      "people -> people\n",
      "who -> who\n",
      "I -> I\n",
      "'d -> would\n",
      "much -> much\n",
      "ratherKick -> ratherkick\n",
      "in -> in\n",
      "the -> the\n",
      "eye?I -> eye?I\n",
      "was -> be\n",
      "happy -> happy\n",
      "in -> in\n",
      "the -> the\n",
      "haze -> haze\n",
      "of -> of\n",
      "a -> a\n",
      "drunken -> drunken\n",
      "hourBut -> hourBut\n",
      "heaven -> heaven\n",
      "knows -> know\n",
      "I -> I\n",
      "'m -> be\n",
      "miserable -> miserable\n",
      "now\"Oh -> now\"Oh\n",
      ", -> ,\n",
      "you -> you\n",
      "'ve -> have\n",
      "been -> be\n",
      "in -> in\n",
      "the -> the\n",
      "house -> house\n",
      "too -> too\n",
      "long -> long\n",
      ", -> ,\n",
      "\" -> \"\n",
      "she -> she\n",
      "saidAnd -> saidand\n",
      "I -> I\n",
      "naturally -> naturally\n",
      "fledIn -> fledIn\n",
      "my -> my\n",
      "life -> life\n",
      ", -> ,\n",
      "oh -> oh\n",
      ", -> ,\n",
      "why -> why\n",
      "do -> do\n",
      "I -> I\n",
      "give -> give\n",
      "valuable -> valuable\n",
      "timeTo -> timeto\n",
      "people -> people\n",
      "who -> who\n",
      "do -> do\n",
      "n't -> not\n",
      "careIf -> careif\n",
      "I -> I\n",
      "live -> live\n",
      "or -> or\n",
      "I -> I\n",
      "die -> die\n",
      "? -> ?\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(lyrics)\n",
    "\n",
    "# Lemmatize each token in the text\n",
    "for token in doc:\n",
    "    print(f\"{token.text} -> {token.lemma_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
